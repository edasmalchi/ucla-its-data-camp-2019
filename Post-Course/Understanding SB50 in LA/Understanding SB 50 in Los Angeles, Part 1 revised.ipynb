{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding SB 50 in LA, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we are going to do our best to recreate the analysis performed by Urban Footprint and summarized in [this blog post](https://urbanfootprint.com/analyzing-sb-50/).\n",
    "\n",
    "Before getting started, make sure that you have the following packages installed: `pandas`, `geopandas`, `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import pandas, geopandas, numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Exercise\n",
    "During the ITS Data Camp Course, we introduced you to the `folium` Python package to display interactive maps right in your browser, made possible because the folium package builds off the leaflet javascript package. For this exercise, we are going to continue to interactively explore spatial data, but we are going to use a different (but very similar) Python package called `ipyleaflet`, which has some additional functionality and plays a little bit better with GeoPandas DataFrames. Take a moment to read more about `ipyleaflet` [here](https://blog.jupyter.org/interactive-gis-in-jupyter-with-ipyleaflet-52f9657fa7a). You will notice that the article mentions `folium` toward the end in a comparison of similar packages.\n",
    "\n",
    "##### Install & Setup of ipyleaflet\n",
    "Follow the instructions (using the conda install) on the [documentation page](https://ipyleaflet.readthedocs.io/en/latest/installation.html#jupyterlab-extension). If the map does not appear, first quit and restart your Jupyter Notebook server. If you are still having difficulty getting the map to appear in your notebook (with everything else loading correctly), take a look at this [SO question](https://gis.stackexchange.com/questions/303261/ipyleaflet-map-object-doesnt-display-in-jupyter-notebook-but-it-gets-created) for the command to enable the extension. You will run this command not in the notebook, but instead in the command prompt (OSX) / anaconda prompt (Windows).\n",
    "\n",
    "##### Test the Install\n",
    "We can confirm that the package was installed properly and the extension was enabled by running the following code, which should display a map with a single point in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyleaflet import Map, Marker\n",
    "\n",
    "center=(34.052235,-118.243683)\n",
    "m = Map(center=center, zoom=10)\n",
    "marker = Marker(location=center, draggable=True)\n",
    "m.add_layer(marker);\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transportation-Based Criteria\n",
    "Let's focus exclusively on service provided by LA Metro for now. There are two different ways that high-quality transit areas were defined in the legislation:\n",
    "1. 1/2 mile from any major rail stop\n",
    "2. 1/4 mile of any high-quality bus stop\n",
    "\n",
    "### Major Rail Stops\n",
    "\n",
    "##### Step 1: Import the Metro Rail Stops\n",
    "Let's start by getting the Metro Rail stops from the [LA City GeoHub](http://geohub.lacity.org/datasets/6679d1ccc3744a7f87f7855e7ce33395_1?geometry=-119.297%2C33.719%2C-117.230%2C34.118). Although we could simply download the data and import it into our folder, let's practice calling the API to get the data. Look for GeoJSON formatted data. Once you make a successful call, save it to our `data/raw` directory.\n",
    "\n",
    "*If this is unfamiliar you can refer to the Day 1 [activity](https://github.com/black-tea/ucla-its-data-camp-2019/blob/master/Day1/Day1-Activity.ipynb) from the 2019 UCLA ITS Data Camp, which contains more detailled instructions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import requests & json packages\n",
    "\n",
    "# TODO: Call the API to get the GeoJSON data, and save to 'data/raw'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Convert to GeoPandas DataFrame\n",
    "Now that we have our data saved to disk, let's go ahead and read it back in and convert it to a GeoPandas dataframe. Because we are going to perform spatial manipulation, we need to make sure we first explicitly state our CRS and then convert it to an appropriate CRS for perform spatial manipulations (I recommend using NAD-83). Take a look at the GeoPandas documentation [here](http://geopandas.org/projections.html) for setting projections and re-projecting data.\n",
    "\n",
    "_For users experiencing an issue with projections: It is likely that somehow your conda installation provided you an older version of pyproj (which you can check). You may need to remove `pyproj` via `conda remove --force pyrpoj` and then reinstall it using pip to get pyproj 2.4.x_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read in to GeoPandas Dataframe\n",
    "rail_stops_gdf = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Examine the DF head to make sure it read in correctly\n",
    "rail_stops_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Examine current CRS\n",
    "\n",
    "\n",
    "# TODO: Re-Project to NAD83\n",
    "rail_stops_nad83 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Buffer 1/2 mile\n",
    "Now that we've re-projected our data, we can go ahead and perform spatial manipulations. Let's create a 1/2 mile buffer around each of the rail stops. _(Pro tip: Each CRS has it's own unit of measurement. Make sure you keep track of what you are currently using!)_ Once we do this, we are going to project it back to WGS84 so that we display it in our map below. We will also want to make sure we save to disk at `data/processed` so that we have it if needed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: buffer 1/2 mi\n",
    "# You could have used either ft or m, as long as it aligns with your CRS\n",
    "rail_stops_halfmi_buff = \n",
    "\n",
    "# TODO: Add original rail data to buffer\n",
    "rail_stops_nad83.geometry = \n",
    "\n",
    "# TODO: Re-project back to WGS84 for map display\n",
    "rail_stop_buff_wgs84 = \n",
    "\n",
    "# TODO: Write out to data/processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the head of the table and confirm that the geometry is showing POLYGON instead of POINT\n",
    "rail_stop_buff_wgs84.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that the buffer looks correct by displaying it (along with the rail stops) on the map, following the example [here](https://ipyleaflet.readthedocs.io/en/latest/api_reference/geodata.html). Since we've already created a gdf and only have one, our code will be simpler than the example. You should see circle polygons that look to have a radius of about 1/2 mi.\n",
    "\n",
    "*If you run the buffering cell multiple times, it may add an additional buffer to the already-buffered polygons. If your unit of measurement seems correct but the polygons are too big, try running the last several cells again to start with a fresh gdf.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Display on map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice work! Let's move on to high-quality bus corridors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Quality Bus Corridors\n",
    "In addition to looking at neighborhoods surrounding rail stops, the legislation also allows for additional density near \"high-quality bus stops\". We will use Urban Footprint's definition of High Quality Bus Corridor as having the following criteria:\n",
    "- less than a 10-minute average headway for three consecutive hours during the morning peak, from 6 to 10 AM, and the evening peak, from 3 to 7 PM.\n",
    "- maintain an average headway below 20 minutes over the course of a weekday from 6 AM to 10 PM and an average headway below 30 minutes on weekends\n",
    "  \n",
    "Because this definition is a bit trickier, we are going to look a bit deeper through Metro's GTFS data. GTFS is a standard developed by Google showing scheduled _(not realtime)_ data. [TransitWiki](https://www.transitwiki.org/TransitWiki/index.php/General_Transit_Feed_Specification) and the [Google developers page](https://developers.google.com/transit/gtfs/) both provide a good overview of the GTFS standard; take some time to look through the documentation on each. Metro provides it's own GTFS data for easy consumption [here](http://developer.metro.net/gtfs-schedule-data/), with a README containing information on each archive.\n",
    "\n",
    "##### Working with GTFS Data\n",
    "There have been numerous development efforts (in multiple programming languages) that make it easier to ingest, analyze, view, and produce GTFS data, many of which you can see [here](https://github.com/CUTR-at-USF/awesome-transit). We will one of the [GTFS libraries](https://github.com/CUTR-at-USF/awesome-transit#gtfs-libraries) to help with the ingestion of our data, but the rest we will do using `GeoPandas`. For this exercise we will be specifically focused on calculating frequency, but feel free to also explore the myriad of tools that have been developed to look at GTFS with another objective in mind.\n",
    "\n",
    "##### Our Plan\n",
    "Transit agencies _may_ optionally publish a table called `frequencies.txt`, which makes it fairly easy to calculate the average frequency at the peak hours. However, you will notice that Metro's archive doesn't contain that file, which means we will need to calculate it ourselves.\n",
    "\n",
    "While looking through [Metro's bus GTFS data](http://developer.metro.net/gtfs-schedule-data/), we decide we are interested in 3 tables: \n",
    "- `trips`: This specifies which routes are bus routes. This is our starting point.\n",
    "- `stop_times`: Combined with trips, we will calculate the arrival frequency using the criteria above to determine the High Quality Routes\n",
    "- `stops`: Once we determine the High Quality Bus Routes, we will want to create the 1/4 mile buffer around the stops\n",
    "\n",
    "##### Step 1: Install `partridge` package\n",
    "We will use the `partridge` package for lightweight parsing of GTFS files (to keep memory overhead low) into `GeoPandas` dataframes that we can then work on for further analysis. Follow the install instructions [here](https://github.com/remix/partridge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Download Data, Get busiest day\n",
    "Begin by downloading the entire LA Metro Bus GTFS zipped file to `data/raw/gtfs_bus-master.zip`. You will notice that the entire size of the folder is around 40MB zipped (and 237MB unzipped!). Whlie data of that size is not an issue storing on disk, loading all of it into [memory (RAM)](https://www.backblaze.com/blog/whats-diff-ram-vs-storage/) will be wasteful and unnecessary. We won't need to unzip the file as `partridge` is designed to directly read from zipped directories.\n",
    "\n",
    "We are going to use `partridge` to get the service IDs that are associated with the busiest day in the GTFS feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import partridge package\n",
    "import partridge as ptg\n",
    "\n",
    "# TODO: Set inpath to zipped directory\n",
    "inpath = \n",
    "\n",
    "# TODO: follow partridge package quickstart example (under install instructions)\n",
    "#       to get date, service_ids for the busiest day\n",
    "date, service_ids = \n",
    "\n",
    "#Specify view using service_ids from busiest day\n",
    "view = {'trips.txt': {'service_id': service_ids}}\n",
    "#Load GTFS feed filtered to service IDs that are associated with the busiest day\n",
    "feed = ptg.load_feed(inpath, view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's in this (now filtered) feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print contents of the feed\n",
    "print([i for i in dir(feed)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that each item listed represents an attribute or method that can be accessed with Python's dot notation. Don't worry about the ones that begin with underscores for now. The attributes we're interested in appear towards the end of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, this cell should return info about the transit agency\n",
    "feed.agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pull stop times from the feed and examine the head of the table\n",
    "stop_times_df = \n",
    "\n",
    "# Examine the head of the table\n",
    "stop_times_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Extract the trips and examine the head of the table\n",
    "trips_df = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Write helper function\n",
    "\n",
    "Write a function that filters the stop_times for only the hours of interest. You'll notice that the times are in seconds, so make sure to account for the appropriate conversion. We will do our calculations based on the `arrival_time`.\n",
    "\n",
    "If you look closely, you'll notice that the max `arrival_time` exceeds the number of seconds in a day. Per [spec](https://developers.google.com/transit/gtfs/reference#stop_timestxt), GTFS arrival times can wrap 'above 24 hours' if the schedule continues overnight into the next day. In this case it only affects early morning hours, before the peak. You can ignore it for the purposes of this exercise, or for an extra challenge think of how to write a function that accounts for this. You may find Python's modulo operator `%` helpful.\n",
    "\n",
    "*If you need a refresher on writing functions in Python, you can go over Geoff Boeing's [exercise](https://github.com/gboeing/urban-data-science/tree/master/04-Python-Loops-Conditionals-Functions) from his urban data science course, or find many other tutorials online such as [this one](http://composingprograms.com/pages/13-defining-new-functions.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a function to filter our stop_times dataframe\n",
    "#       by a beginning hour (inclusive) and end hour (exclusive)\n",
    "#       using methods in the Pandas library\n",
    "def filter_stoptimes_byhour(df, begin_hour, end_hour):\n",
    "    ''' \n",
    "    Filter a GTFS DF by start/end hour\n",
    "    \n",
    "    :param df DataFrame: Pandas DF of stop_times from partridge\n",
    "    :param begin_hour int: begin hour (inclusive), 0-23\n",
    "    :param end_hour int: end hour (not inclusive), 1-24\n",
    "    returns: Filtered stop_times df\n",
    "    '''\n",
    "\n",
    "\n",
    "# Compare the length of raw & filtered dataframes for comparison\n",
    "print(f'The unfiltered DF contains {len(stop_times_df)} rows.')\n",
    "print(f'Between 7-10am, there are {len(filter_stoptimes_byhour(stop_times_df, 7, 10))} rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Calculate Stop Average Frequency at Each Peak\n",
    "As a reminder, The first criterion for High-Quality Bus Corridors is that the routes \"have less than a 10-minute average headway for three consecutive hours\" in the AM peak. During this step, we are going to filter for all the routes that have more than 18 scheduled arrivals in the 3 hour period between 7-10am.\n",
    "\n",
    "With that helper function complete, we are ready to write the aggregation pipeline to compute the average frequency for each hour in the `stop_times` dataframe. _This next step will involve a fairly heavy use of the `pandas` library. I recommend [brushing up](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html) on some of the most common operations, especially [groupby](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#groupby), if the steps below seem intimidating. Also, note that the `pandas` library often has multiple methods that essentially perform the same function, so how you complete the next step will look a little different depending on the methods you choose._ \n",
    "  \n",
    "We will calculate the frequency through the following steps:\n",
    "1. Filter for hours by using our new function `filter_stoptimes_byhour`.\n",
    "2. Join `stop_times` to `trips`, joining on the `trip_id`.\n",
    "3. Group by the following:\n",
    "    - `route_id`\n",
    "    - `direction_id`\n",
    "    - `stop_id`\n",
    "    - `service_id`\n",
    "4. Summarize by count into a new column called `count`\n",
    "5. Filter for rows that have `count` >= 18.\n",
    "6. Create a new column `am_headway` that shows the average headway (in minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform aggregation & computation steps from above\n",
    "\n",
    "\n",
    "# Exmaine the length and head of the new aggregated table\n",
    "print(len(stop_freq_am_df))\n",
    "stop_freq_am_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to providing minimum of a 10 minute headway for the AM Peak, the stop must also meet the same minimum frequency for the PM Peak. Again, although the legislation gives a little bit of leeway with a 4 hour window, we are going to pick the hours of 4-7pm (the same hours used in the UF analysis). The new column should be named `pm_headway`.\n",
    "  \n",
    "_Hint: Make sure to use 24 hour format in your `filter_stoptimes_byhour` function_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Perform same steps for PM Peak\n",
    "\n",
    "\n",
    "# TODO: Exmaine the length and head of the new aggregated table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Filter for those stops that meet both headway conditions\n",
    "The stop must meet both AM and PM frequency criteria in order to be eligible for SB50 consideration. If you looked at the length of the two dataframes, you'll see that there is a difference in the number of rows, so we know that getting the intersection of both tables will produce (at most) the length of the smaller dataframe. \n",
    "\n",
    "To perform this filter, we need to do the following:\n",
    "- Use a join to filter for rows with a `route_id` and `stop_id` that meet both criteria (`direction_id` can be different).\n",
    "- Because a route has multiple directions, it is possible that a route would have frequent service in both directions at the same time, resulting in either of the tables having multiple rows with the same `route_id` and `stop_id`. Performing the join will result in the [cartesian product](https://www.geeksforgeeks.org/sql-join-cartesian-join-self-join/) of the matches. It shouldn't be too many, but make sure that we remove any rows with duplicate (`route_id`, `stop_id`) after the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find route_id, stop_id that meet both criteria\n",
    "\n",
    "\n",
    "# TODO: Print the length and head of the new table for confirmation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the high-frequency stops, we can join it to  the `stops` feed to get the geometry objects for the buffer. Let's keep only the `stop_id`, `stop_name`, `am_headway`, `pm_headway`, `stop_lat`, `stop_lon` columns, dropping the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pull stops from the feed and examine the head of the table\n",
    "\n",
    "\n",
    "# TODO: Join stops with stop_freq_df and drop unnecessary columns\n",
    "\n",
    "\n",
    "# TODO: Exmaine the head of the final df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7: Create 1/4 mi. buffers around High-Quality Transit Corridor Stops\n",
    "After the join, follow the same general steps as with the rail stations to create buffers around each of these stops (but this time for 1/4 mi. instead of 1/2 mi.). \n",
    "\n",
    "Since we're creating a gdf from a dataframe with lat/lon coordinates and not a GeoJSON, there will be no CRS initially specified. Don't forget to project to WGS84, either when generating the gdf or before reprojecting to NAD83.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convert to gdf, project, buffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and display the buffers on a map using `ipyleaflet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create map object, add buffers, and display\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as with the rail stop, let's go ahead and save this buffered geometry to `data/processed/hqbus_stop_buff_wgs84.geojson`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write out to data/processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Be Continued...\n",
    "In the next part, we will add the non-transportation criteria for areas that were considered for SB50. Stay tuned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
